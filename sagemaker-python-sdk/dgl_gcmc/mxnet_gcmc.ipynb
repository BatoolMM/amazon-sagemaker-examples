{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GCMC using the DGL with MXNet backend on SageMaker\n",
    "The **SageMaker Python SDK** makes it easy to train DGL models. In this example, we train [Graph Convolutional Matrix Completion](https://arxiv.org/abs/1706.02263) network using the [DMLC DGL API](https://github.com/dmlc/dgl.git) and the [MovieLens dataset](https://grouplens.org/datasets/movielens/). Currently we support 3 datasets:\n",
    " * MovieLens 100K Dataset, MovieLens 100K movie ratings. Stable benchmark dataset. 100,000 ratings from 1000 users on 1700 movies.\n",
    " * MovieLens 1M Dataset, MovieLens 1M movie ratings. Stable benchmark dataset. 1 million ratings from 6000 users on 4000 movies.\n",
    " * MovieLens 10M Dataset, MovieLens 10M movie ratings. Stable benchmark dataset. 10 million ratings and 100,000 tag applications applied to 10,000 movies by 72,000 users.\n",
    "\n",
    "### Prepare\n",
    "First we need to install necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y boto3\n",
    "!conda install -c anaconda -y botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "# Setup session\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "# Location to put your custom code.\n",
    "custom_code_upload_location = 'customcode'\n",
    "\n",
    "# Location where results of model training are saved.\n",
    "model_artifacts_location = 's3://{}/artifacts'.format(bucket)\n",
    "\n",
    "# IAM execution role that gives SageMaker access to resources in your AWS account.\n",
    "# We can use the SageMaker Python SDK to get the role from our notebook environment. \n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training script\n",
    "The train.py script provides all the code we need for training a SageMaker model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring Your Own GCMC Docker Image\n",
    "AWS provides basic docker images in https://docs.aws.amazon.com/dlami/latest/devguide/deep-learning-containers-images.html. For both pytorch 1.3 and mxnet 1.6, DGL is pre-installed. As this example needs additional dependancies, we provide a dockerfile to build a new image. You should build a GCMC specific docker image and push it into your ECR.\n",
    "\n",
    "Note: Do change the GCMC.Dockerfile if you are in different region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "echo $account\n",
    "region=$(aws configure get region)\n",
    "\n",
    "docker_name=sagemaker-dgl-gcmc\n",
    "\n",
    "$(aws ecr get-login --no-include-email --region ${region} --registry-ids 763104351884)\n",
    "docker build -t $docker_name -f GCMC.Dockerfile .\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${docker_name}:latest\"\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${docker_name}\" > /dev/null 2>&1\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${docker_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "docker tag ${docker_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker's  estimator class\n",
    "The SageMaker Estimator allows us to run single machine in SageMaker, using CPU or GPU-based instances.\n",
    "\n",
    "When we create the estimator, we pass in the filename of our training script, the name of our IAM execution role. We also provide a few other parameters. train_instance_count and train_instance_type determine the number and type of SageMaker instances that will be used for the training job. The hyperparameters parameter is a dict of values that will be passed to your training script -- you can see how to access these values in the train.py script above.\n",
    "\n",
    "The entrypoint of sagemaker docker (e.g., dgllib/dgl-sagemaker-gpu:dgl_0.4_mxnet_1.5.1) is a **train** script under /usr/bin/. The **train** script inside dgl docker image provided in the above will try to get the **real entrypoint** from hyperparameters and run the **real entrypoint** under **'training-code' data channel** (/opt/ml/input/data/training-code/) .\n",
    "\n",
    "In this example, we will upload the whole code base (including train.py) into SageMaker container and run the GCMC training using MovieLens dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.mxnet.estimator import MXNet\n",
    "\n",
    "# Set target dgl-docker name\n",
    "docker_name='sagemaker-dgl-gcmc'\n",
    "\n",
    "CODE_PATH = '../dgl_gcmc'\n",
    "CODE_ENTRY = 'train.py'\n",
    "#code_location = sess.upload_data(CODE_PATH, bucket=bucket, key_prefix=custom_code_upload_location)\n",
    "\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account, region, docker_name)\n",
    "print(image)\n",
    "\n",
    "params = {}\n",
    "params['data_name'] = 'ml-1m'\n",
    "# set output to SageMaker ML output\n",
    "params['save_dir'] = '/opt/ml/model/'\n",
    "estimator = MXNet(entry_point=CODE_ENTRY,\n",
    "                  source_dir=CODE_PATH,\n",
    "                        role=role, \n",
    "                        train_instance_count=1, \n",
    "                        train_instance_type='ml.p3.2xlarge',\n",
    "                        image_name=image,\n",
    "                        hyperparameters=params,\n",
    "                        sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Training Job\n",
    "After we've constructed our Estimator object, we can fit it using sagemaker (The dataset will be automatically downloaded). Below we run SageMaker training on one channels: training-code, the code to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "You can get the model training output from the Sagemaker Console by searching for the training task and looking for the address of 'S3 model artifact'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
