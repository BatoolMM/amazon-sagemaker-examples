{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training SageMaker Models using the DGL with MXNet backend\n",
    "The **SageMaker Python SDK** makes it easy to train DGL models. In this example, we train a simple graph neural network using the [DMLC DGL API](https://github.com/dmlc/dgl.git) and the [cora dataset](https://relational.fit.cvut.cz/dataset/CORA). The cora dataset describes a citation network. The cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. The task at hand is to train a node classification model using Cora dataset. \n",
    "\n",
    "For more details about Graph Neural Network and this example please refer to https://docs.dgl.ai/en/latest/tutorials/models/1_gnn/1_gcn.html\n",
    "\n",
    "### Prepare\n",
    "First we need to install necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.5.12\n",
      "  latest version: 4.7.12\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/mxnet_p36\n",
      "\n",
      "  added / updated specs: \n",
      "    - boto3\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    boto3-1.10.19              |             py_0          91 KB\n",
      "    botocore-1.13.19           |             py_0         3.3 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.4 MB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    boto3:    1.9.234-py_0  --> 1.10.19-py_0\n",
      "    botocore: 1.12.234-py_0 --> 1.13.19-py_0\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "boto3-1.10.19        | 91 KB     | ##################################### | 100% \n",
      "botocore-1.13.19     | 3.3 MB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.5.12\n",
      "  latest version: 4.7.12\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/mxnet_p36\n",
      "\n",
      "  added / updated specs: \n",
      "    - botocore\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2019.10.16 |                0         131 KB  anaconda\n",
      "    botocore-1.13.19           |             py_0         3.3 MB  anaconda\n",
      "    openssl-1.0.2t             |       h7b6447c_1         3.1 MB  anaconda\n",
      "    certifi-2019.9.11          |           py36_0         154 KB  anaconda\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         6.8 MB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    botocore:        1.13.19-py_0      --> 1.13.19-py_0      anaconda\n",
      "    ca-certificates: 2019.10.16-0      --> 2019.10.16-0      anaconda\n",
      "    certifi:         2019.9.11-py36_0  --> 2019.9.11-py36_0  anaconda\n",
      "    openssl:         1.0.2t-h7b6447c_1 --> 1.0.2t-h7b6447c_1 anaconda\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "ca-certificates-2019 | 131 KB    | ##################################### | 100% \n",
      "botocore-1.13.19     | 3.3 MB    | ##################################### | 100% \n",
      "openssl-1.0.2t       | 3.1 MB    | ##################################### | 100% \n",
      "certifi-2019.9.11    | 154 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -y boto3\n",
    "!conda install -c anaconda -y botocore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "We need to define a few variables that will be needed later in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "# Setup session\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "# Location to put your custom code.\n",
    "custom_code_upload_location = 'customcode'\n",
    "\n",
    "# IAM execution role that gives SageMaker access to resources in your AWS account.\n",
    "# We can use the SageMaker Python SDK to get the role from our notebook environment. \n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training script\n",
    "The mxnet_gcn.py script provides all the code we need for training a SageMaker model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\r\n",
      "# coding: utf-8\r\n",
      "\r\n",
      "\"\"\"GCN using DGL nn package\r\n",
      "References:\r\n",
      "- Semi-Supervised Classification with Graph Convolutional Networks\r\n",
      "- Paper: https://arxiv.org/abs/1609.02907\r\n",
      "- Code: https://github.com/tkipf/gcn\r\n",
      "\"\"\"\r\n",
      "import mxnet as mx\r\n",
      "from mxnet import gluon\r\n",
      "import os\r\n",
      "import argparse\r\n",
      "import dgl\r\n",
      "from dgl.nn.mxnet import GraphConv\r\n",
      "\r\n",
      "import time\r\n",
      "import json\r\n",
      "import numpy as np\r\n",
      "from mxnet import gluon\r\n",
      "\r\n",
      "from dgl import DGLGraph\r\n",
      "from dgl.data import register_data_args, load_data\r\n",
      "\r\n",
      "import collections\r\n",
      "class GCN(gluon.Block):\r\n",
      "    def __init__(self,\r\n",
      "                 g,\r\n",
      "                 in_feats,\r\n",
      "                 n_hidden,\r\n",
      "                 n_classes,\r\n",
      "                 n_layers,\r\n",
      "                 activation,\r\n",
      "                 dropout):\r\n",
      "        super(GCN, self).__init__()\r\n",
      "        self.g = g\r\n",
      "        self.layers = gluon.nn.Sequential()\r\n",
      "        # input layer\r\n",
      "        self.layers.add(GraphConv(in_feats, n_hidden, activation=activation))\r\n",
      "        # hidden layers\r\n",
      "        for i in range(n_layers - 1):\r\n",
      "            self.layers.add(GraphConv(n_hidden, n_hidden, activation=activation))\r\n",
      "        # output layer\r\n",
      "        self.layers.add(GraphConv(n_hidden, n_classes))\r\n",
      "        self.dropout = gluon.nn.Dropout(rate=dropout)\r\n",
      "\r\n",
      "    def forward(self, features):\r\n",
      "        h = features\r\n",
      "        for i, layer in enumerate(self.layers):\r\n",
      "            if i != 0:\r\n",
      "                h = self.dropout(h)\r\n",
      "            h = layer(self.g, h)\r\n",
      "        return h\r\n",
      "\r\n",
      "def evaluate(model, features, labels, mask):\r\n",
      "    pred = model(features).argmax(axis=1)\r\n",
      "    accuracy = ((pred == labels) * mask).sum() / mask.sum().asscalar()\r\n",
      "    return accuracy.asscalar()\r\n",
      "\r\n",
      "def main(args):\r\n",
      "    # load and preprocess dataset\r\n",
      "    data = load_data(args)\r\n",
      "    features = mx.nd.array(data.features)\r\n",
      "    labels = mx.nd.array(data.labels)\r\n",
      "    train_mask = mx.nd.array(data.train_mask)\r\n",
      "    val_mask = mx.nd.array(data.val_mask)\r\n",
      "    test_mask = mx.nd.array(data.test_mask)\r\n",
      "    in_feats = features.shape[1]\r\n",
      "    n_classes = data.num_labels\r\n",
      "    n_edges = data.graph.number_of_edges()\r\n",
      "    print(\"\"\"----Data statistics------'\r\n",
      "      #Edges %d\r\n",
      "      #Classes %d\r\n",
      "      #Train samples %d\r\n",
      "      #Val samples %d\r\n",
      "      #Test samples %d\"\"\" %\r\n",
      "          (n_edges, n_classes,\r\n",
      "              train_mask.sum().asscalar(),\r\n",
      "              val_mask.sum().asscalar(),\r\n",
      "              test_mask.sum().asscalar()))\r\n",
      "\r\n",
      "    if args.gpu < 0:\r\n",
      "        cuda = False\r\n",
      "        ctx = mx.cpu(0)\r\n",
      "    else:\r\n",
      "        cuda = True\r\n",
      "        ctx = mx.gpu(args.gpu)\r\n",
      "\r\n",
      "    features = features.as_in_context(ctx)\r\n",
      "    labels = labels.as_in_context(ctx)\r\n",
      "    train_mask = train_mask.as_in_context(ctx)\r\n",
      "    val_mask = val_mask.as_in_context(ctx)\r\n",
      "    test_mask = test_mask.as_in_context(ctx)\r\n",
      "\r\n",
      "    # create GCN model\r\n",
      "    g = data.graph\r\n",
      "    if args.self_loop:\r\n",
      "        g.remove_edges_from(g.selfloop_edges())\r\n",
      "        g.add_edges_from(zip(g.nodes(), g.nodes()))\r\n",
      "    g = DGLGraph(g)\r\n",
      "    # normalization\r\n",
      "    degs = g.in_degrees().astype('float32')\r\n",
      "    norm = mx.nd.power(degs, -0.5)\r\n",
      "    if cuda:\r\n",
      "        norm = norm.as_in_context(ctx)\r\n",
      "    g.ndata['norm'] = mx.nd.expand_dims(norm, 1)\r\n",
      "\r\n",
      "    model = GCN(g,\r\n",
      "                in_feats,\r\n",
      "                args.n_hidden,\r\n",
      "                n_classes,\r\n",
      "                args.n_layers,\r\n",
      "                mx.nd.relu,\r\n",
      "                args.dropout)\r\n",
      "    model.initialize(ctx=ctx)\r\n",
      "    n_train_samples = train_mask.sum().asscalar()\r\n",
      "    loss_fcn = gluon.loss.SoftmaxCELoss()\r\n",
      "\r\n",
      "    # use optimizer\r\n",
      "    print(model.collect_params())\r\n",
      "    trainer = gluon.Trainer(model.collect_params(), 'adam',\r\n",
      "            {'learning_rate': args.lr, 'wd': args.weight_decay})\r\n",
      "\r\n",
      "    # initialize graph\r\n",
      "    dur = []\r\n",
      "    for epoch in range(args.n_epochs):\r\n",
      "        if epoch >= 3:\r\n",
      "            t0 = time.time()\r\n",
      "        # forward\r\n",
      "        with mx.autograd.record():\r\n",
      "            pred = model(features)\r\n",
      "            loss = loss_fcn(pred, labels, mx.nd.expand_dims(train_mask, 1))\r\n",
      "            loss = loss.sum() / n_train_samples\r\n",
      "\r\n",
      "        loss.backward()\r\n",
      "        trainer.step(batch_size=1)\r\n",
      "\r\n",
      "        if epoch >= 3:\r\n",
      "            loss.asscalar()\r\n",
      "            dur.append(time.time() - t0)\r\n",
      "            acc = evaluate(model, features, labels, val_mask)\r\n",
      "            print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | Accuracy {:.4f} | \"\r\n",
      "                  \"ETputs(KTEPS) {:.2f}\". format(\r\n",
      "                epoch, np.mean(dur), loss.asscalar(), acc, n_edges / np.mean(dur) / 1000))\r\n",
      "\r\n",
      "    # test set accuracy\r\n",
      "    acc = evaluate(model, features, labels, test_mask)\r\n",
      "    print(\"Test accuracy {:.2%}\".format(acc))\r\n",
      "\r\n",
      "    model.save_parameters(args.save_path)\r\n",
      "\r\n",
      "def parse_args():\r\n",
      "    parser = argparse.ArgumentParser(description='GCN')\r\n",
      "    register_data_args(parser)\r\n",
      "    parser.add_argument(\"--dropout\", type=float, default=0.5,\r\n",
      "            help=\"dropout probability\")\r\n",
      "    parser.add_argument(\"--gpu\", type=int, default=-1,\r\n",
      "            help=\"gpu\")\r\n",
      "    parser.add_argument(\"--lr\", type=float, default=3e-2,\r\n",
      "            help=\"learning rate\")\r\n",
      "    parser.add_argument(\"--n-epochs\", type=int, default=200,\r\n",
      "            help=\"number of training epochs\")\r\n",
      "    parser.add_argument(\"--n-hidden\", type=int, default=16,\r\n",
      "            help=\"number of hidden gcn units\")\r\n",
      "    parser.add_argument(\"--n-layers\", type=int, default=1,\r\n",
      "            help=\"number of hidden gcn layers\")\r\n",
      "    parser.add_argument(\"--weight-decay\", type=float, default=5e-4,\r\n",
      "            help=\"Weight for L2 loss\")\r\n",
      "    parser.add_argument(\"--self-loop\", action='store_true',\r\n",
      "            help=\"graph self-loop (default=False)\")\r\n",
      "    parser.add_argument(\"--save-path\", type=str, default='./model/gcn.params',\r\n",
      "            help=\"path to save model\")\r\n",
      "    parser.set_defaults(self_loop=False)\r\n",
      "\r\n",
      "    return parser.parse_args()\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    args = parse_args()\r\n",
      "    num_gpus = int(os.environ['SM_NUM_GPUS'])\r\n",
      "    if num_gpus == 0:\r\n",
      "        args.gpu = -1\r\n",
      "    else:\r\n",
      "        args.gpu = 0\r\n",
      "\r\n",
      "    path = str(os.environ['SM_MODEL_DIR'])\r\n",
      "    args.save_path = os.path.join(path, 'gcn.params')\r\n",
      "    print(args)\r\n",
      "    main(args)\r\n"
     ]
    }
   ],
   "source": [
    "!cat mxnet_gcn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker's  estimator class\n",
    "The SageMaker Estimator allows us to run single machine in SageMaker, using CPU or GPU-based instances.\n",
    "\n",
    "When we create the estimator, we pass in the filename of our training script, the name of our IAM execution role. We also provide a few other parameters. train_instance_count and train_instance_type determine the number and type of SageMaker instances that will be used for the training job. The hyperparameters parameter is a dict of values that will be passed to your training script -- you can see how to access these values in the mxnet_gcn.py script above.\n",
    "\n",
    "Here we can use the official docker image for this example, please see https://github.com/aws/sagemaker-mxnet-container for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No framework_version specified, defaulting to version 1.2. This is not the latest supported version. If you would like to use version 1.4.1, please add framework_version=1.4.1 to your constructor.\n",
      "The Python 2 mxnet images will be soon deprecated and may not be supported for newer upcoming versions of the mxnet images.\n",
      "Please set the argument \"py_version='py3'\" to use the Python 3 mxnet image.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397262719838.dkr.ecr.us-east-2.amazonaws.com/beta-mxnet-training:1.6.0-py3-gpu-build\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.mxnet.estimator import MXNet\n",
    "\n",
    "CODE_PATH = 'mxnet_gcn.py'\n",
    "\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "docker_name = 'beta-mxnet-training'\n",
    "docker_tag = '1.6.0-py3-gpu-build'\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/{}:{}'.format(account, region, docker_name, docker_tag)\n",
    "print(image)\n",
    "\n",
    "params = {}\n",
    "params['dataset'] = 'cora'\n",
    "estimator = MXNet(entry_point=CODE_PATH,\n",
    "                        role=role, \n",
    "                        train_instance_count=1, \n",
    "                        train_instance_type='ml.p3.2xlarge',\n",
    "                        image_name=image,\n",
    "                        hyperparameters=params,\n",
    "                        sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Training Job\n",
    "After we've constructed our Estimator object, we can fit it using sagemaker (The dataset will be automatically downloaded). Below we run SageMaker training on one channels: training-code, the code to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-27 01:19:57 Starting - Starting the training job...\n",
      "2019-11-27 01:19:58 Starting - Launching requested ML instances......\n",
      "2019-11-27 01:21:22 Starting - Preparing the instances for training......\n",
      "2019-11-27 01:22:04 Downloading - Downloading input data\n",
      "2019-11-27 01:22:04 Training - Downloading the training image.........\n",
      "2019-11-27 01:23:45 Training - Training image download completed. Training in progress.\u001b[31m2019-11-27 01:23:46,223 sagemaker-containers INFO     Imported framework sagemaker_mxnet_container.training\u001b[0m\n",
      "\u001b[31m2019-11-27 01:23:46,249 sagemaker_mxnet_container.training INFO     MXNet training environment: {'SM_HOSTS': '[\"algo-1\"]', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_HPS': '{\"dataset\":\"cora\"}', 'SM_USER_ENTRY_POINT': 'mxnet_gcn.py', 'SM_FRAMEWORK_PARAMS': '{}', 'SM_RESOURCE_CONFIG': '{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}', 'SM_INPUT_DATA_CONFIG': '{}', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_CHANNELS': '[]', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODULE_NAME': 'mxnet_gcn', 'SM_LOG_LEVEL': '20', 'SM_FRAMEWORK_MODULE': 'sagemaker_mxnet_container.training:main', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_NUM_CPUS': '8', 'SM_NUM_GPUS': '1', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_MODULE_DIR': 's3://sagemaker-us-east-2-397262719838/beta-mxnet-training-2019-11-27-01-19-56-747/source/sourcedir.tar.gz', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"dataset\":\"cora\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"beta-mxnet-training-2019-11-27-01-19-56-747\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-397262719838/beta-mxnet-training-2019-11-27-01-19-56-747/source/sourcedir.tar.gz\",\"module_name\":\"mxnet_gcn\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mxnet_gcn.py\"}', 'SM_USER_ARGS': '[\"--dataset\",\"cora\"]', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_HP_DATASET': 'cora'}\u001b[0m\n",
      "\u001b[31m2019-11-27 01:23:46,503 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2019-11-27 01:23:46,503 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2019-11-27 01:23:46,504 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2019-11-27 01:23:46,504 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/usr/local/bin/python3.6 -m pip install -U . \u001b[0m\n",
      "\u001b[31mProcessing /tmp/tmpci1ja87k/module_dir\u001b[0m\n",
      "\u001b[31mInstalling collected packages: default-user-module-name\n",
      "    Running setup.py install for default-user-module-name: started\n",
      "    Running setup.py install for default-user-module-name: finished with status 'done'\u001b[0m\n",
      "\u001b[31mSuccessfully installed default-user-module-name-1.0.0\u001b[0m\n",
      "\u001b[31m2019-11-27 01:23:48,477 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_mxnet_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dataset\": \"cora\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"beta-mxnet-training-2019-11-27-01-19-56-747\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-397262719838/beta-mxnet-training-2019-11-27-01-19-56-747/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mxnet_gcn\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mxnet_gcn.py\"\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[31mSM_HPS={\"dataset\":\"cora\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=mxnet_gcn.py\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=mxnet_gcn\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_mxnet_container.training:main\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-us-east-2-397262719838/beta-mxnet-training-2019-11-27-01-19-56-747/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"dataset\":\"cora\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"beta-mxnet-training-2019-11-27-01-19-56-747\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-397262719838/beta-mxnet-training-2019-11-27-01-19-56-747/source/sourcedir.tar.gz\",\"module_name\":\"mxnet_gcn\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mxnet_gcn.py\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--dataset\",\"cora\"]\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_HP_DATASET=cora\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python36.zip:/usr/local/lib/python3.6:/usr/local/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/local/bin/python3.6 mxnet_gcn.py --dataset cora\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mNamespace(dataset='cora', dropout=0.5, gpu=0, lr=0.03, n_epochs=200, n_hidden=16, n_layers=1, save_path='/opt/ml/model/gcn.params', self_loop=False, syn_gnp_n=1000, syn_gnp_p=0.0, syn_nclasses=10, syn_nfeats=500, syn_seed=42, syn_test_ratio=0.5, syn_train_ratio=0.1, syn_type='gnp', syn_val_ratio=0.2, weight_decay=0.0005)\u001b[0m\n",
      "\u001b[31mDownloading /root/.dgl/cora.zip from https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/cora_raw.zip...\u001b[0m\n",
      "\u001b[31mExtracting file to /root/.dgl/cora\u001b[0m\n",
      "\u001b[31m----Data statistics------'\n",
      "      #Edges 10556\n",
      "      #Classes 7\n",
      "      #Train samples 140\n",
      "      #Val samples 300\n",
      "      #Test samples 1000\u001b[0m\n",
      "\u001b[31mgcn0_ (\n",
      "  Parameter graphconv0_weight (shape=(1433, 16), dtype=<class 'numpy.float32'>)\n",
      "  Parameter graphconv0_bias (shape=(16,), dtype=<class 'numpy.float32'>)\n",
      "  Parameter graphconv1_weight (shape=(16, 7), dtype=<class 'numpy.float32'>)\n",
      "  Parameter graphconv1_bias (shape=(7,), dtype=<class 'numpy.float32'>)\u001b[0m\n",
      "\u001b[31m)\u001b[0m\n",
      "\u001b[31mEpoch 00003 | Time(s) 0.0236 | Loss 1.8374 | Accuracy 0.4133 | ETputs(KTEPS) 448.21\u001b[0m\n",
      "\u001b[31mEpoch 00004 | Time(s) 0.0157 | Loss 1.7915 | Accuracy 0.4400 | ETputs(KTEPS) 671.06\u001b[0m\n",
      "\u001b[31mEpoch 00005 | Time(s) 0.0127 | Loss 1.7643 | Accuracy 0.4500 | ETputs(KTEPS) 828.73\u001b[0m\n",
      "\u001b[31mEpoch 00006 | Time(s) 0.0114 | Loss 1.7458 | Accuracy 0.4433 | ETputs(KTEPS) 925.40\u001b[0m\n",
      "\u001b[31mEpoch 00007 | Time(s) 0.0104 | Loss 1.7251 | Accuracy 0.4300 | ETputs(KTEPS) 1010.74\u001b[0m\n",
      "\u001b[31mEpoch 00008 | Time(s) 0.0099 | Loss 1.7326 | Accuracy 0.4033 | ETputs(KTEPS) 1061.15\u001b[0m\n",
      "\u001b[31mEpoch 00009 | Time(s) 0.0096 | Loss 1.6747 | Accuracy 0.3967 | ETputs(KTEPS) 1099.55\u001b[0m\n",
      "\u001b[31mEpoch 00010 | Time(s) 0.0093 | Loss 1.6523 | Accuracy 0.3967 | ETputs(KTEPS) 1136.27\u001b[0m\n",
      "\u001b[31mEpoch 00011 | Time(s) 0.0093 | Loss 1.6224 | Accuracy 0.3967 | ETputs(KTEPS) 1139.11\u001b[0m\n",
      "\u001b[31mEpoch 00012 | Time(s) 0.0091 | Loss 1.5866 | Accuracy 0.4200 | ETputs(KTEPS) 1162.90\u001b[0m\n",
      "\u001b[31mEpoch 00013 | Time(s) 0.0089 | Loss 1.5655 | Accuracy 0.4700 | ETputs(KTEPS) 1183.03\u001b[0m\n",
      "\u001b[31mEpoch 00014 | Time(s) 0.0088 | Loss 1.5474 | Accuracy 0.5067 | ETputs(KTEPS) 1198.50\u001b[0m\n",
      "\u001b[31mEpoch 00015 | Time(s) 0.0086 | Loss 1.4998 | Accuracy 0.5267 | ETputs(KTEPS) 1222.85\u001b[0m\n",
      "\u001b[31mEpoch 00016 | Time(s) 0.0086 | Loss 1.5013 | Accuracy 0.5300 | ETputs(KTEPS) 1233.55\u001b[0m\n",
      "\u001b[31mEpoch 00017 | Time(s) 0.0084 | Loss 1.4807 | Accuracy 0.5333 | ETputs(KTEPS) 1252.08\u001b[0m\n",
      "\u001b[31mEpoch 00018 | Time(s) 0.0083 | Loss 1.4388 | Accuracy 0.5267 | ETputs(KTEPS) 1269.51\u001b[0m\n",
      "\u001b[31mEpoch 00019 | Time(s) 0.0083 | Loss 1.4064 | Accuracy 0.5100 | ETputs(KTEPS) 1277.71\u001b[0m\n",
      "\u001b[31mEpoch 00020 | Time(s) 0.0082 | Loss 1.3855 | Accuracy 0.4967 | ETputs(KTEPS) 1284.90\u001b[0m\n",
      "\u001b[31mEpoch 00021 | Time(s) 0.0082 | Loss 1.3689 | Accuracy 0.4833 | ETputs(KTEPS) 1280.92\u001b[0m\n",
      "\u001b[31mEpoch 00022 | Time(s) 0.0082 | Loss 1.3475 | Accuracy 0.5067 | ETputs(KTEPS) 1288.62\u001b[0m\n",
      "\u001b[31mEpoch 00023 | Time(s) 0.0082 | Loss 1.3064 | Accuracy 0.5233 | ETputs(KTEPS) 1290.51\u001b[0m\n",
      "\u001b[31mEpoch 00024 | Time(s) 0.0081 | Loss 1.2801 | Accuracy 0.5433 | ETputs(KTEPS) 1297.63\u001b[0m\n",
      "\u001b[31mEpoch 00025 | Time(s) 0.0081 | Loss 1.2717 | Accuracy 0.5600 | ETputs(KTEPS) 1303.50\u001b[0m\n",
      "\u001b[31mEpoch 00026 | Time(s) 0.0081 | Loss 1.2102 | Accuracy 0.6000 | ETputs(KTEPS) 1307.33\u001b[0m\n",
      "\u001b[31mEpoch 00027 | Time(s) 0.0080 | Loss 1.1980 | Accuracy 0.6267 | ETputs(KTEPS) 1314.61\u001b[0m\n",
      "\u001b[31mEpoch 00028 | Time(s) 0.0080 | Loss 1.1668 | Accuracy 0.6467 | ETputs(KTEPS) 1314.33\u001b[0m\n",
      "\u001b[31mEpoch 00029 | Time(s) 0.0080 | Loss 1.1157 | Accuracy 0.6533 | ETputs(KTEPS) 1318.69\u001b[0m\n",
      "\u001b[31mEpoch 00030 | Time(s) 0.0080 | Loss 1.1287 | Accuracy 0.6567 | ETputs(KTEPS) 1322.98\u001b[0m\n",
      "\u001b[31mEpoch 00031 | Time(s) 0.0080 | Loss 1.0977 | Accuracy 0.6533 | ETputs(KTEPS) 1326.79\u001b[0m\n",
      "\u001b[31mEpoch 00032 | Time(s) 0.0079 | Loss 1.0583 | Accuracy 0.6600 | ETputs(KTEPS) 1330.55\u001b[0m\n",
      "\u001b[31mEpoch 00033 | Time(s) 0.0079 | Loss 1.0486 | Accuracy 0.6733 | ETputs(KTEPS) 1329.06\u001b[0m\n",
      "\u001b[31mEpoch 00034 | Time(s) 0.0079 | Loss 1.0035 | Accuracy 0.6800 | ETputs(KTEPS) 1334.70\u001b[0m\n",
      "\u001b[31mEpoch 00035 | Time(s) 0.0079 | Loss 0.9915 | Accuracy 0.6800 | ETputs(KTEPS) 1338.03\u001b[0m\n",
      "\u001b[31mEpoch 00036 | Time(s) 0.0079 | Loss 0.9913 | Accuracy 0.6833 | ETputs(KTEPS) 1344.15\u001b[0m\n",
      "\u001b[31mEpoch 00037 | Time(s) 0.0078 | Loss 0.9381 | Accuracy 0.6867 | ETputs(KTEPS) 1351.49\u001b[0m\n",
      "\u001b[31mEpoch 00038 | Time(s) 0.0078 | Loss 0.9179 | Accuracy 0.6867 | ETputs(KTEPS) 1358.35\u001b[0m\n",
      "\u001b[31mEpoch 00039 | Time(s) 0.0077 | Loss 0.9224 | Accuracy 0.6833 | ETputs(KTEPS) 1365.99\u001b[0m\n",
      "\u001b[31mEpoch 00040 | Time(s) 0.0078 | Loss 0.9454 | Accuracy 0.6967 | ETputs(KTEPS) 1360.74\u001b[0m\n",
      "\u001b[31mEpoch 00041 | Time(s) 0.0077 | Loss 0.9011 | Accuracy 0.6967 | ETputs(KTEPS) 1365.53\u001b[0m\n",
      "\u001b[31mEpoch 00042 | Time(s) 0.0077 | Loss 0.8586 | Accuracy 0.7000 | ETputs(KTEPS) 1371.22\u001b[0m\n",
      "\u001b[31mEpoch 00043 | Time(s) 0.0077 | Loss 0.8343 | Accuracy 0.7100 | ETputs(KTEPS) 1377.03\u001b[0m\n",
      "\u001b[31mEpoch 00044 | Time(s) 0.0077 | Loss 0.9092 | Accuracy 0.7133 | ETputs(KTEPS) 1379.13\u001b[0m\n",
      "\u001b[31mEpoch 00045 | Time(s) 0.0076 | Loss 0.8437 | Accuracy 0.7233 | ETputs(KTEPS) 1379.90\u001b[0m\n",
      "\u001b[31mEpoch 00046 | Time(s) 0.0076 | Loss 0.7637 | Accuracy 0.7333 | ETputs(KTEPS) 1384.11\u001b[0m\n",
      "\u001b[31mEpoch 00047 | Time(s) 0.0076 | Loss 0.8158 | Accuracy 0.7367 | ETputs(KTEPS) 1385.75\u001b[0m\n",
      "\u001b[31mEpoch 00048 | Time(s) 0.0076 | Loss 0.8010 | Accuracy 0.7400 | ETputs(KTEPS) 1389.39\u001b[0m\n",
      "\u001b[31mEpoch 00049 | Time(s) 0.0076 | Loss 0.7808 | Accuracy 0.7433 | ETputs(KTEPS) 1394.20\u001b[0m\n",
      "\u001b[31mEpoch 00050 | Time(s) 0.0076 | Loss 0.7538 | Accuracy 0.7400 | ETputs(KTEPS) 1397.52\u001b[0m\n",
      "\u001b[31mEpoch 00051 | Time(s) 0.0075 | Loss 0.6948 | Accuracy 0.7467 | ETputs(KTEPS) 1401.54\u001b[0m\n",
      "\u001b[31mEpoch 00052 | Time(s) 0.0075 | Loss 0.7600 | Accuracy 0.7633 | ETputs(KTEPS) 1402.06\u001b[0m\n",
      "\u001b[31mEpoch 00053 | Time(s) 0.0075 | Loss 0.7066 | Accuracy 0.7700 | ETputs(KTEPS) 1405.84\u001b[0m\n",
      "\u001b[31mEpoch 00054 | Time(s) 0.0075 | Loss 0.6529 | Accuracy 0.7700 | ETputs(KTEPS) 1407.21\u001b[0m\n",
      "\u001b[31mEpoch 00055 | Time(s) 0.0075 | Loss 0.7005 | Accuracy 0.7700 | ETputs(KTEPS) 1410.81\u001b[0m\n",
      "\u001b[31mEpoch 00056 | Time(s) 0.0075 | Loss 0.6677 | Accuracy 0.7733 | ETputs(KTEPS) 1414.29\u001b[0m\n",
      "\u001b[31mEpoch 00057 | Time(s) 0.0075 | Loss 0.7413 | Accuracy 0.7767 | ETputs(KTEPS) 1416.57\u001b[0m\n",
      "\u001b[31mEpoch 00058 | Time(s) 0.0075 | Loss 0.6820 | Accuracy 0.7667 | ETputs(KTEPS) 1408.41\u001b[0m\n",
      "\u001b[31mEpoch 00059 | Time(s) 0.0075 | Loss 0.6711 | Accuracy 0.7667 | ETputs(KTEPS) 1405.94\u001b[0m\n",
      "\u001b[31mEpoch 00060 | Time(s) 0.0075 | Loss 0.6327 | Accuracy 0.7633 | ETputs(KTEPS) 1406.20\u001b[0m\n",
      "\u001b[31mEpoch 00061 | Time(s) 0.0075 | Loss 0.6542 | Accuracy 0.7700 | ETputs(KTEPS) 1406.10\u001b[0m\n",
      "\u001b[31mEpoch 00062 | Time(s) 0.0075 | Loss 0.6415 | Accuracy 0.7667 | ETputs(KTEPS) 1405.95\u001b[0m\n",
      "\u001b[31mEpoch 00063 | Time(s) 0.0075 | Loss 0.6717 | Accuracy 0.7833 | ETputs(KTEPS) 1406.64\u001b[0m\n",
      "\u001b[31mEpoch 00064 | Time(s) 0.0075 | Loss 0.6306 | Accuracy 0.7800 | ETputs(KTEPS) 1404.23\u001b[0m\n",
      "\u001b[31mEpoch 00065 | Time(s) 0.0075 | Loss 0.6336 | Accuracy 0.7800 | ETputs(KTEPS) 1403.91\u001b[0m\n",
      "\u001b[31mEpoch 00066 | Time(s) 0.0075 | Loss 0.6040 | Accuracy 0.7800 | ETputs(KTEPS) 1403.13\u001b[0m\n",
      "\u001b[31mEpoch 00067 | Time(s) 0.0075 | Loss 0.5527 | Accuracy 0.7867 | ETputs(KTEPS) 1403.10\u001b[0m\n",
      "\u001b[31mEpoch 00068 | Time(s) 0.0075 | Loss 0.5896 | Accuracy 0.7833 | ETputs(KTEPS) 1402.96\u001b[0m\n",
      "\u001b[31mEpoch 00069 | Time(s) 0.0075 | Loss 0.5578 | Accuracy 0.7833 | ETputs(KTEPS) 1403.27\u001b[0m\n",
      "\u001b[31mEpoch 00070 | Time(s) 0.0075 | Loss 0.5598 | Accuracy 0.7833 | ETputs(KTEPS) 1404.27\u001b[0m\n",
      "\u001b[31mEpoch 00071 | Time(s) 0.0075 | Loss 0.5686 | Accuracy 0.7833 | ETputs(KTEPS) 1400.15\u001b[0m\n",
      "\u001b[31mEpoch 00072 | Time(s) 0.0075 | Loss 0.5728 | Accuracy 0.7900 | ETputs(KTEPS) 1400.44\u001b[0m\n",
      "\u001b[31mEpoch 00073 | Time(s) 0.0075 | Loss 0.5823 | Accuracy 0.8033 | ETputs(KTEPS) 1400.49\u001b[0m\n",
      "\u001b[31mEpoch 00074 | Time(s) 0.0075 | Loss 0.5124 | Accuracy 0.7933 | ETputs(KTEPS) 1400.69\u001b[0m\n",
      "\u001b[31mEpoch 00075 | Time(s) 0.0075 | Loss 0.5455 | Accuracy 0.7933 | ETputs(KTEPS) 1400.62\u001b[0m\n",
      "\u001b[31mEpoch 00076 | Time(s) 0.0075 | Loss 0.5767 | Accuracy 0.7967 | ETputs(KTEPS) 1400.65\u001b[0m\n",
      "\u001b[31mEpoch 00077 | Time(s) 0.0075 | Loss 0.5436 | Accuracy 0.7967 | ETputs(KTEPS) 1401.05\u001b[0m\n",
      "\u001b[31mEpoch 00078 | Time(s) 0.0075 | Loss 0.5002 | Accuracy 0.7867 | ETputs(KTEPS) 1401.52\u001b[0m\n",
      "\u001b[31mEpoch 00079 | Time(s) 0.0075 | Loss 0.4829 | Accuracy 0.7900 | ETputs(KTEPS) 1398.89\u001b[0m\n",
      "\u001b[31mEpoch 00080 | Time(s) 0.0075 | Loss 0.5064 | Accuracy 0.7900 | ETputs(KTEPS) 1399.98\u001b[0m\n",
      "\u001b[31mEpoch 00081 | Time(s) 0.0075 | Loss 0.4724 | Accuracy 0.7933 | ETputs(KTEPS) 1399.03\u001b[0m\n",
      "\u001b[31mEpoch 00082 | Time(s) 0.0075 | Loss 0.4749 | Accuracy 0.7967 | ETputs(KTEPS) 1399.28\u001b[0m\n",
      "\u001b[31mEpoch 00083 | Time(s) 0.0075 | Loss 0.5223 | Accuracy 0.8000 | ETputs(KTEPS) 1398.97\u001b[0m\n",
      "\u001b[31mEpoch 00084 | Time(s) 0.0075 | Loss 0.4467 | Accuracy 0.8033 | ETputs(KTEPS) 1398.74\u001b[0m\n",
      "\u001b[31mEpoch 00085 | Time(s) 0.0075 | Loss 0.5582 | Accuracy 0.8000 | ETputs(KTEPS) 1399.35\u001b[0m\n",
      "\u001b[31mEpoch 00086 | Time(s) 0.0076 | Loss 0.5320 | Accuracy 0.8100 | ETputs(KTEPS) 1397.45\u001b[0m\n",
      "\u001b[31mEpoch 00087 | Time(s) 0.0076 | Loss 0.4681 | Accuracy 0.8067 | ETputs(KTEPS) 1397.28\u001b[0m\n",
      "\u001b[31mEpoch 00088 | Time(s) 0.0076 | Loss 0.5208 | Accuracy 0.8100 | ETputs(KTEPS) 1397.77\u001b[0m\n",
      "\u001b[31mEpoch 00089 | Time(s) 0.0075 | Loss 0.5126 | Accuracy 0.8100 | ETputs(KTEPS) 1398.20\u001b[0m\n",
      "\u001b[31mEpoch 00090 | Time(s) 0.0075 | Loss 0.4613 | Accuracy 0.8067 | ETputs(KTEPS) 1400.73\u001b[0m\n",
      "\u001b[31mEpoch 00091 | Time(s) 0.0075 | Loss 0.5230 | Accuracy 0.8033 | ETputs(KTEPS) 1401.34\u001b[0m\n",
      "\u001b[31mEpoch 00092 | Time(s) 0.0075 | Loss 0.4866 | Accuracy 0.8133 | ETputs(KTEPS) 1402.96\u001b[0m\n",
      "\u001b[31mEpoch 00093 | Time(s) 0.0075 | Loss 0.5231 | Accuracy 0.8167 | ETputs(KTEPS) 1406.04\u001b[0m\n",
      "\u001b[31mEpoch 00094 | Time(s) 0.0075 | Loss 0.4852 | Accuracy 0.8233 | ETputs(KTEPS) 1408.46\u001b[0m\n",
      "\u001b[31mEpoch 00095 | Time(s) 0.0075 | Loss 0.5041 | Accuracy 0.8000 | ETputs(KTEPS) 1410.00\u001b[0m\n",
      "\u001b[31mEpoch 00096 | Time(s) 0.0075 | Loss 0.4468 | Accuracy 0.7933 | ETputs(KTEPS) 1409.76\u001b[0m\n",
      "\u001b[31mEpoch 00097 | Time(s) 0.0075 | Loss 0.5158 | Accuracy 0.7833 | ETputs(KTEPS) 1412.34\u001b[0m\n",
      "\u001b[31mEpoch 00098 | Time(s) 0.0075 | Loss 0.4424 | Accuracy 0.7867 | ETputs(KTEPS) 1412.53\u001b[0m\n",
      "\u001b[31mEpoch 00099 | Time(s) 0.0075 | Loss 0.4595 | Accuracy 0.7900 | ETputs(KTEPS) 1413.17\u001b[0m\n",
      "\u001b[31mEpoch 00100 | Time(s) 0.0075 | Loss 0.4669 | Accuracy 0.7967 | ETputs(KTEPS) 1414.66\u001b[0m\n",
      "\u001b[31mEpoch 00101 | Time(s) 0.0075 | Loss 0.4718 | Accuracy 0.8100 | ETputs(KTEPS) 1415.36\u001b[0m\n",
      "\u001b[31mEpoch 00102 | Time(s) 0.0074 | Loss 0.4257 | Accuracy 0.8233 | ETputs(KTEPS) 1416.99\u001b[0m\n",
      "\u001b[31mEpoch 00103 | Time(s) 0.0074 | Loss 0.4300 | Accuracy 0.8267 | ETputs(KTEPS) 1417.26\u001b[0m\n",
      "\u001b[31mEpoch 00104 | Time(s) 0.0074 | Loss 0.5293 | Accuracy 0.8067 | ETputs(KTEPS) 1417.21\u001b[0m\n",
      "\u001b[31mEpoch 00105 | Time(s) 0.0074 | Loss 0.4534 | Accuracy 0.8033 | ETputs(KTEPS) 1417.57\u001b[0m\n",
      "\u001b[31mEpoch 00106 | Time(s) 0.0074 | Loss 0.4597 | Accuracy 0.8033 | ETputs(KTEPS) 1418.99\u001b[0m\n",
      "\u001b[31mEpoch 00107 | Time(s) 0.0074 | Loss 0.4235 | Accuracy 0.7967 | ETputs(KTEPS) 1421.33\u001b[0m\n",
      "\u001b[31mEpoch 00108 | Time(s) 0.0074 | Loss 0.4838 | Accuracy 0.8067 | ETputs(KTEPS) 1422.71\u001b[0m\n",
      "\u001b[31mEpoch 00109 | Time(s) 0.0074 | Loss 0.4333 | Accuracy 0.8067 | ETputs(KTEPS) 1422.42\u001b[0m\n",
      "\u001b[31mEpoch 00110 | Time(s) 0.0074 | Loss 0.4472 | Accuracy 0.8133 | ETputs(KTEPS) 1421.91\u001b[0m\n",
      "\u001b[31mEpoch 00111 | Time(s) 0.0074 | Loss 0.3933 | Accuracy 0.8067 | ETputs(KTEPS) 1420.66\u001b[0m\n",
      "\u001b[31mEpoch 00112 | Time(s) 0.0074 | Loss 0.4027 | Accuracy 0.8100 | ETputs(KTEPS) 1421.83\u001b[0m\n",
      "\u001b[31mEpoch 00113 | Time(s) 0.0074 | Loss 0.4675 | Accuracy 0.8000 | ETputs(KTEPS) 1421.44\u001b[0m\n",
      "\u001b[31mEpoch 00114 | Time(s) 0.0074 | Loss 0.4712 | Accuracy 0.8067 | ETputs(KTEPS) 1421.09\u001b[0m\n",
      "\u001b[31mEpoch 00115 | Time(s) 0.0074 | Loss 0.4214 | Accuracy 0.7967 | ETputs(KTEPS) 1420.93\u001b[0m\n",
      "\u001b[31mEpoch 00116 | Time(s) 0.0074 | Loss 0.3816 | Accuracy 0.8067 | ETputs(KTEPS) 1419.59\u001b[0m\n",
      "\u001b[31mEpoch 00117 | Time(s) 0.0074 | Loss 0.3983 | Accuracy 0.8167 | ETputs(KTEPS) 1419.90\u001b[0m\n",
      "\u001b[31mEpoch 00118 | Time(s) 0.0074 | Loss 0.4425 | Accuracy 0.8133 | ETputs(KTEPS) 1419.23\u001b[0m\n",
      "\u001b[31mEpoch 00119 | Time(s) 0.0074 | Loss 0.4122 | Accuracy 0.8067 | ETputs(KTEPS) 1419.43\u001b[0m\n",
      "\u001b[31mEpoch 00120 | Time(s) 0.0074 | Loss 0.4170 | Accuracy 0.8233 | ETputs(KTEPS) 1419.38\u001b[0m\n",
      "\u001b[31mEpoch 00121 | Time(s) 0.0074 | Loss 0.3986 | Accuracy 0.8200 | ETputs(KTEPS) 1419.09\u001b[0m\n",
      "\u001b[31mEpoch 00122 | Time(s) 0.0074 | Loss 0.4162 | Accuracy 0.8233 | ETputs(KTEPS) 1420.34\u001b[0m\n",
      "\u001b[31mEpoch 00123 | Time(s) 0.0074 | Loss 0.4421 | Accuracy 0.8167 | ETputs(KTEPS) 1420.08\u001b[0m\n",
      "\u001b[31mEpoch 00124 | Time(s) 0.0074 | Loss 0.3961 | Accuracy 0.8100 | ETputs(KTEPS) 1420.29\u001b[0m\n",
      "\u001b[31mEpoch 00125 | Time(s) 0.0074 | Loss 0.4046 | Accuracy 0.8067 | ETputs(KTEPS) 1421.01\u001b[0m\n",
      "\u001b[31mEpoch 00126 | Time(s) 0.0074 | Loss 0.4499 | Accuracy 0.8000 | ETputs(KTEPS) 1422.08\u001b[0m\n",
      "\u001b[31mEpoch 00127 | Time(s) 0.0074 | Loss 0.4799 | Accuracy 0.8067 | ETputs(KTEPS) 1423.26\u001b[0m\n",
      "\u001b[31mEpoch 00128 | Time(s) 0.0074 | Loss 0.4592 | Accuracy 0.8167 | ETputs(KTEPS) 1423.93\u001b[0m\n",
      "\u001b[31mEpoch 00129 | Time(s) 0.0074 | Loss 0.4000 | Accuracy 0.8267 | ETputs(KTEPS) 1422.73\u001b[0m\n",
      "\u001b[31mEpoch 00130 | Time(s) 0.0074 | Loss 0.4001 | Accuracy 0.8233 | ETputs(KTEPS) 1423.95\u001b[0m\n",
      "\u001b[31mEpoch 00131 | Time(s) 0.0074 | Loss 0.4108 | Accuracy 0.8233 | ETputs(KTEPS) 1424.68\u001b[0m\n",
      "\u001b[31mEpoch 00132 | Time(s) 0.0074 | Loss 0.4678 | Accuracy 0.8133 | ETputs(KTEPS) 1423.56\u001b[0m\n",
      "\u001b[31mEpoch 00133 | Time(s) 0.0074 | Loss 0.4113 | Accuracy 0.8167 | ETputs(KTEPS) 1423.64\u001b[0m\n",
      "\u001b[31mEpoch 00134 | Time(s) 0.0074 | Loss 0.3957 | Accuracy 0.8200 | ETputs(KTEPS) 1424.43\u001b[0m\n",
      "\u001b[31mEpoch 00135 | Time(s) 0.0074 | Loss 0.4579 | Accuracy 0.8167 | ETputs(KTEPS) 1425.22\u001b[0m\n",
      "\u001b[31mEpoch 00136 | Time(s) 0.0074 | Loss 0.3857 | Accuracy 0.8233 | ETputs(KTEPS) 1425.81\u001b[0m\n",
      "\u001b[31mEpoch 00137 | Time(s) 0.0074 | Loss 0.4668 | Accuracy 0.8167 | ETputs(KTEPS) 1427.26\u001b[0m\n",
      "\u001b[31mEpoch 00138 | Time(s) 0.0074 | Loss 0.4270 | Accuracy 0.8133 | ETputs(KTEPS) 1426.48\u001b[0m\n",
      "\u001b[31mEpoch 00139 | Time(s) 0.0074 | Loss 0.4385 | Accuracy 0.8133 | ETputs(KTEPS) 1428.13\u001b[0m\n",
      "\u001b[31mEpoch 00140 | Time(s) 0.0074 | Loss 0.4282 | Accuracy 0.8067 | ETputs(KTEPS) 1428.74\u001b[0m\n",
      "\u001b[31mEpoch 00141 | Time(s) 0.0074 | Loss 0.3543 | Accuracy 0.8167 | ETputs(KTEPS) 1428.33\u001b[0m\n",
      "\u001b[31mEpoch 00142 | Time(s) 0.0074 | Loss 0.3376 | Accuracy 0.8133 | ETputs(KTEPS) 1428.47\u001b[0m\n",
      "\u001b[31mEpoch 00143 | Time(s) 0.0074 | Loss 0.4245 | Accuracy 0.8233 | ETputs(KTEPS) 1427.50\u001b[0m\n",
      "\u001b[31mEpoch 00144 | Time(s) 0.0074 | Loss 0.3827 | Accuracy 0.8267 | ETputs(KTEPS) 1426.07\u001b[0m\n",
      "\u001b[31mEpoch 00145 | Time(s) 0.0074 | Loss 0.4083 | Accuracy 0.8200 | ETputs(KTEPS) 1425.84\u001b[0m\n",
      "\u001b[31mEpoch 00146 | Time(s) 0.0074 | Loss 0.4079 | Accuracy 0.8200 | ETputs(KTEPS) 1426.71\u001b[0m\n",
      "\u001b[31mEpoch 00147 | Time(s) 0.0074 | Loss 0.3786 | Accuracy 0.8200 | ETputs(KTEPS) 1427.83\u001b[0m\n",
      "\u001b[31mEpoch 00148 | Time(s) 0.0074 | Loss 0.4035 | Accuracy 0.8167 | ETputs(KTEPS) 1428.29\u001b[0m\n",
      "\u001b[31mEpoch 00149 | Time(s) 0.0074 | Loss 0.4294 | Accuracy 0.8167 | ETputs(KTEPS) 1429.25\u001b[0m\n",
      "\u001b[31mEpoch 00150 | Time(s) 0.0074 | Loss 0.4337 | Accuracy 0.8200 | ETputs(KTEPS) 1429.68\u001b[0m\n",
      "\u001b[31mEpoch 00151 | Time(s) 0.0074 | Loss 0.3575 | Accuracy 0.8133 | ETputs(KTEPS) 1430.72\u001b[0m\n",
      "\u001b[31mEpoch 00152 | Time(s) 0.0074 | Loss 0.3872 | Accuracy 0.8167 | ETputs(KTEPS) 1432.14\u001b[0m\n",
      "\u001b[31mEpoch 00153 | Time(s) 0.0074 | Loss 0.4172 | Accuracy 0.8100 | ETputs(KTEPS) 1432.40\u001b[0m\n",
      "\u001b[31mEpoch 00154 | Time(s) 0.0074 | Loss 0.3818 | Accuracy 0.8067 | ETputs(KTEPS) 1433.71\u001b[0m\n",
      "\u001b[31mEpoch 00155 | Time(s) 0.0074 | Loss 0.3892 | Accuracy 0.8167 | ETputs(KTEPS) 1434.43\u001b[0m\n",
      "\u001b[31mEpoch 00156 | Time(s) 0.0074 | Loss 0.4036 | Accuracy 0.8000 | ETputs(KTEPS) 1435.01\u001b[0m\n",
      "\u001b[31mEpoch 00157 | Time(s) 0.0073 | Loss 0.3826 | Accuracy 0.8033 | ETputs(KTEPS) 1436.45\u001b[0m\n",
      "\u001b[31mEpoch 00158 | Time(s) 0.0074 | Loss 0.4377 | Accuracy 0.8067 | ETputs(KTEPS) 1435.35\u001b[0m\n",
      "\u001b[31mEpoch 00159 | Time(s) 0.0074 | Loss 0.4182 | Accuracy 0.8033 | ETputs(KTEPS) 1435.31\u001b[0m\n",
      "\u001b[31mEpoch 00160 | Time(s) 0.0074 | Loss 0.3849 | Accuracy 0.8200 | ETputs(KTEPS) 1435.16\u001b[0m\n",
      "\u001b[31mEpoch 00161 | Time(s) 0.0074 | Loss 0.3981 | Accuracy 0.8167 | ETputs(KTEPS) 1435.78\u001b[0m\n",
      "\u001b[31mEpoch 00162 | Time(s) 0.0074 | Loss 0.4094 | Accuracy 0.8167 | ETputs(KTEPS) 1435.13\u001b[0m\n",
      "\u001b[31mEpoch 00163 | Time(s) 0.0074 | Loss 0.3754 | Accuracy 0.8133 | ETputs(KTEPS) 1435.65\u001b[0m\n",
      "\u001b[31mEpoch 00164 | Time(s) 0.0073 | Loss 0.3119 | Accuracy 0.8133 | ETputs(KTEPS) 1437.04\u001b[0m\n",
      "\u001b[31mEpoch 00165 | Time(s) 0.0073 | Loss 0.3950 | Accuracy 0.8100 | ETputs(KTEPS) 1438.19\u001b[0m\n",
      "\u001b[31mEpoch 00166 | Time(s) 0.0073 | Loss 0.3861 | Accuracy 0.8133 | ETputs(KTEPS) 1438.21\u001b[0m\n",
      "\u001b[31mEpoch 00167 | Time(s) 0.0073 | Loss 0.3884 | Accuracy 0.8167 | ETputs(KTEPS) 1437.84\u001b[0m\n",
      "\u001b[31mEpoch 00168 | Time(s) 0.0073 | Loss 0.3471 | Accuracy 0.8300 | ETputs(KTEPS) 1437.42\u001b[0m\n",
      "\u001b[31mEpoch 00169 | Time(s) 0.0073 | Loss 0.4064 | Accuracy 0.8200 | ETputs(KTEPS) 1437.36\u001b[0m\n",
      "\u001b[31mEpoch 00170 | Time(s) 0.0073 | Loss 0.4067 | Accuracy 0.8200 | ETputs(KTEPS) 1437.30\u001b[0m\n",
      "\u001b[31mEpoch 00171 | Time(s) 0.0073 | Loss 0.3487 | Accuracy 0.8100 | ETputs(KTEPS) 1436.80\u001b[0m\n",
      "\u001b[31mEpoch 00172 | Time(s) 0.0073 | Loss 0.4084 | Accuracy 0.8100 | ETputs(KTEPS) 1436.97\u001b[0m\n",
      "\u001b[31mEpoch 00173 | Time(s) 0.0073 | Loss 0.3748 | Accuracy 0.8167 | ETputs(KTEPS) 1436.99\u001b[0m\n",
      "\u001b[31mEpoch 00174 | Time(s) 0.0074 | Loss 0.4267 | Accuracy 0.8167 | ETputs(KTEPS) 1435.44\u001b[0m\n",
      "\u001b[31mEpoch 00175 | Time(s) 0.0074 | Loss 0.3466 | Accuracy 0.8167 | ETputs(KTEPS) 1435.62\u001b[0m\n",
      "\u001b[31mEpoch 00176 | Time(s) 0.0073 | Loss 0.3645 | Accuracy 0.8200 | ETputs(KTEPS) 1436.47\u001b[0m\n",
      "\u001b[31mEpoch 00177 | Time(s) 0.0073 | Loss 0.3422 | Accuracy 0.8100 | ETputs(KTEPS) 1437.57\u001b[0m\n",
      "\u001b[31mEpoch 00178 | Time(s) 0.0073 | Loss 0.3813 | Accuracy 0.8100 | ETputs(KTEPS) 1437.92\u001b[0m\n",
      "\u001b[31mEpoch 00179 | Time(s) 0.0073 | Loss 0.4400 | Accuracy 0.8033 | ETputs(KTEPS) 1438.75\u001b[0m\n",
      "\u001b[31mEpoch 00180 | Time(s) 0.0073 | Loss 0.4011 | Accuracy 0.8100 | ETputs(KTEPS) 1439.86\u001b[0m\n",
      "\u001b[31mEpoch 00181 | Time(s) 0.0073 | Loss 0.4082 | Accuracy 0.8167 | ETputs(KTEPS) 1439.71\u001b[0m\n",
      "\u001b[31mEpoch 00182 | Time(s) 0.0073 | Loss 0.3925 | Accuracy 0.8167 | ETputs(KTEPS) 1439.94\u001b[0m\n",
      "\u001b[31mEpoch 00183 | Time(s) 0.0073 | Loss 0.3975 | Accuracy 0.8167 | ETputs(KTEPS) 1439.26\u001b[0m\n",
      "\u001b[31mEpoch 00184 | Time(s) 0.0073 | Loss 0.3542 | Accuracy 0.8167 | ETputs(KTEPS) 1439.05\u001b[0m\n",
      "\u001b[31mEpoch 00185 | Time(s) 0.0073 | Loss 0.3463 | Accuracy 0.8133 | ETputs(KTEPS) 1439.09\u001b[0m\n",
      "\u001b[31mEpoch 00186 | Time(s) 0.0073 | Loss 0.3951 | Accuracy 0.8133 | ETputs(KTEPS) 1439.00\u001b[0m\n",
      "\u001b[31mEpoch 00187 | Time(s) 0.0073 | Loss 0.3378 | Accuracy 0.8133 | ETputs(KTEPS) 1438.89\u001b[0m\n",
      "\u001b[31mEpoch 00188 | Time(s) 0.0073 | Loss 0.3443 | Accuracy 0.8067 | ETputs(KTEPS) 1438.32\u001b[0m\n",
      "\u001b[31mEpoch 00189 | Time(s) 0.0073 | Loss 0.3660 | Accuracy 0.8133 | ETputs(KTEPS) 1438.96\u001b[0m\n",
      "\u001b[31mEpoch 00190 | Time(s) 0.0073 | Loss 0.3757 | Accuracy 0.8100 | ETputs(KTEPS) 1439.97\u001b[0m\n",
      "\u001b[31mEpoch 00191 | Time(s) 0.0073 | Loss 0.3215 | Accuracy 0.8133 | ETputs(KTEPS) 1440.65\u001b[0m\n",
      "\u001b[31mEpoch 00192 | Time(s) 0.0073 | Loss 0.3774 | Accuracy 0.8167 | ETputs(KTEPS) 1441.45\u001b[0m\n",
      "\u001b[31mEpoch 00193 | Time(s) 0.0073 | Loss 0.3634 | Accuracy 0.8133 | ETputs(KTEPS) 1441.79\u001b[0m\n",
      "\u001b[31mEpoch 00194 | Time(s) 0.0073 | Loss 0.3432 | Accuracy 0.8067 | ETputs(KTEPS) 1442.92\u001b[0m\n",
      "\u001b[31mEpoch 00195 | Time(s) 0.0073 | Loss 0.3596 | Accuracy 0.8133 | ETputs(KTEPS) 1442.87\u001b[0m\n",
      "\u001b[31mEpoch 00196 | Time(s) 0.0073 | Loss 0.3484 | Accuracy 0.8133 | ETputs(KTEPS) 1442.74\u001b[0m\n",
      "\u001b[31mEpoch 00197 | Time(s) 0.0073 | Loss 0.3333 | Accuracy 0.8100 | ETputs(KTEPS) 1442.80\u001b[0m\n",
      "\u001b[31mEpoch 00198 | Time(s) 0.0073 | Loss 0.3699 | Accuracy 0.8067 | ETputs(KTEPS) 1442.02\u001b[0m\n",
      "\u001b[31mEpoch 00199 | Time(s) 0.0073 | Loss 0.3452 | Accuracy 0.8100 | ETputs(KTEPS) 1442.00\u001b[0m\n",
      "\u001b[31mTest accuracy 80.50%\u001b[0m\n",
      "\u001b[31m2019-11-27 01:24:10,014 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-11-27 01:24:16 Uploading - Uploading generated training model\n",
      "2019-11-27 01:24:16 Completed - Training job completed\n",
      "Training seconds: 138\n",
      "Billable seconds: 138\n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "You can get the model training output from the Sagemaker Console by searching for the training task and looking for the address of 'S3 model artifact'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
