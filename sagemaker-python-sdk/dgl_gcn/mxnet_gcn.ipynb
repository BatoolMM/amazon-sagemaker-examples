{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training SageMaker Models using the DGL with MXNet backend\n",
    "The **SageMaker Python SDK** makes it easy to train DGL models. In this example, we train a simple graph neural network using the [DMLC DGL API](https://github.com/dmlc/dgl.git) and the [cora dataset](https://relational.fit.cvut.cz/dataset/CORA). The cora dataset describes a citation network. The cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. The task at hand is to train a node classification model using Cora dataset. \n",
    "\n",
    "For more details about Graph Neural Network and this example please refer to https://docs.dgl.ai/en/latest/tutorials/models/1_gnn/1_gcn.html\n",
    "\n",
    "### Prepare\n",
    "First we need to install necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.5.12\n",
      "  latest version: 4.7.12\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/DGL_py36_mxnet1.5\n",
      "\n",
      "  added / updated specs: \n",
      "    - boto3\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    boto3-1.10.19              |             py_0          91 KB\n",
      "    botocore-1.13.19           |             py_0         3.3 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.4 MB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    boto3:    1.10.13-py_0          --> 1.10.19-py_0\n",
      "    botocore: 1.13.13-py_0 anaconda --> 1.13.19-py_0\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "boto3-1.10.19        | 91 KB     | ##################################### | 100% \n",
      "botocore-1.13.19     | 3.3 MB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.5.12\n",
      "  latest version: 4.7.12\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/DGL_py36_mxnet1.5\n",
      "\n",
      "  added / updated specs: \n",
      "    - botocore\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    botocore-1.13.19           |             py_0         3.3 MB  anaconda\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    botocore: 1.13.19-py_0 --> 1.13.19-py_0 anaconda\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "botocore-1.13.19     | 3.3 MB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -y boto3\n",
    "!conda install -c anaconda -y botocore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "We need to define a few variables that will be needed later in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "# Setup session\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "# Location to put your custom code.\n",
    "custom_code_upload_location = 'customcode'\n",
    "\n",
    "# IAM execution role that gives SageMaker access to resources in your AWS account.\n",
    "# We can use the SageMaker Python SDK to get the role from our notebook environment. \n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training script\n",
    "The mxnet_gcn.py script provides all the code we need for training a SageMaker model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\r\n",
      "# coding: utf-8\r\n",
      "\r\n",
      "\"\"\"GCN using DGL nn package\r\n",
      "References:\r\n",
      "- Semi-Supervised Classification with Graph Convolutional Networks\r\n",
      "- Paper: https://arxiv.org/abs/1609.02907\r\n",
      "- Code: https://github.com/tkipf/gcn\r\n",
      "\"\"\"\r\n",
      "import mxnet as mx\r\n",
      "from mxnet import gluon\r\n",
      "import os\r\n",
      "import argparse\r\n",
      "import dgl\r\n",
      "from dgl.nn.mxnet import GraphConv\r\n",
      "\r\n",
      "import time\r\n",
      "import json\r\n",
      "import numpy as np\r\n",
      "from mxnet import gluon\r\n",
      "\r\n",
      "from dgl import DGLGraph\r\n",
      "from dgl.data import register_data_args, load_data\r\n",
      "\r\n",
      "import collections\r\n",
      "class GCN(gluon.Block):\r\n",
      "    def __init__(self,\r\n",
      "                 g,\r\n",
      "                 in_feats,\r\n",
      "                 n_hidden,\r\n",
      "                 n_classes,\r\n",
      "                 n_layers,\r\n",
      "                 activation,\r\n",
      "                 dropout):\r\n",
      "        super(GCN, self).__init__()\r\n",
      "        self.g = g\r\n",
      "        self.layers = gluon.nn.Sequential()\r\n",
      "        # input layer\r\n",
      "        self.layers.add(GraphConv(in_feats, n_hidden, activation=activation))\r\n",
      "        # hidden layers\r\n",
      "        for i in range(n_layers - 1):\r\n",
      "            self.layers.add(GraphConv(n_hidden, n_hidden, activation=activation))\r\n",
      "        # output layer\r\n",
      "        self.layers.add(GraphConv(n_hidden, n_classes))\r\n",
      "        self.dropout = gluon.nn.Dropout(rate=dropout)\r\n",
      "\r\n",
      "    def forward(self, features):\r\n",
      "        h = features\r\n",
      "        for i, layer in enumerate(self.layers):\r\n",
      "            if i != 0:\r\n",
      "                h = self.dropout(h)\r\n",
      "            h = layer(self.g, h)\r\n",
      "        return h\r\n",
      "\r\n",
      "def evaluate(model, features, labels, mask):\r\n",
      "    pred = model(features).argmax(axis=1)\r\n",
      "    accuracy = ((pred == labels) * mask).sum() / mask.sum().asscalar()\r\n",
      "    return accuracy.asscalar()\r\n",
      "\r\n",
      "def main(args):\r\n",
      "    # load and preprocess dataset\r\n",
      "    data = load_data(args)\r\n",
      "    features = mx.nd.array(data.features)\r\n",
      "    labels = mx.nd.array(data.labels)\r\n",
      "    train_mask = mx.nd.array(data.train_mask)\r\n",
      "    val_mask = mx.nd.array(data.val_mask)\r\n",
      "    test_mask = mx.nd.array(data.test_mask)\r\n",
      "    in_feats = features.shape[1]\r\n",
      "    n_classes = data.num_labels\r\n",
      "    n_edges = data.graph.number_of_edges()\r\n",
      "    print(\"\"\"----Data statistics------'\r\n",
      "      #Edges %d\r\n",
      "      #Classes %d\r\n",
      "      #Train samples %d\r\n",
      "      #Val samples %d\r\n",
      "      #Test samples %d\"\"\" %\r\n",
      "          (n_edges, n_classes,\r\n",
      "              train_mask.sum().asscalar(),\r\n",
      "              val_mask.sum().asscalar(),\r\n",
      "              test_mask.sum().asscalar()))\r\n",
      "\r\n",
      "    if args.gpu < 0:\r\n",
      "        cuda = False\r\n",
      "        ctx = mx.cpu(0)\r\n",
      "    else:\r\n",
      "        cuda = True\r\n",
      "        ctx = mx.gpu(args.gpu)\r\n",
      "\r\n",
      "    features = features.as_in_context(ctx)\r\n",
      "    labels = labels.as_in_context(ctx)\r\n",
      "    train_mask = train_mask.as_in_context(ctx)\r\n",
      "    val_mask = val_mask.as_in_context(ctx)\r\n",
      "    test_mask = test_mask.as_in_context(ctx)\r\n",
      "\r\n",
      "    # create GCN model\r\n",
      "    g = data.graph\r\n",
      "    if args.self_loop:\r\n",
      "        g.remove_edges_from(g.selfloop_edges())\r\n",
      "        g.add_edges_from(zip(g.nodes(), g.nodes()))\r\n",
      "    g = DGLGraph(g)\r\n",
      "    # normalization\r\n",
      "    degs = g.in_degrees().astype('float32')\r\n",
      "    norm = mx.nd.power(degs, -0.5)\r\n",
      "    if cuda:\r\n",
      "        norm = norm.as_in_context(ctx)\r\n",
      "    g.ndata['norm'] = mx.nd.expand_dims(norm, 1)\r\n",
      "\r\n",
      "    model = GCN(g,\r\n",
      "                in_feats,\r\n",
      "                args.n_hidden,\r\n",
      "                n_classes,\r\n",
      "                args.n_layers,\r\n",
      "                mx.nd.relu,\r\n",
      "                args.dropout)\r\n",
      "    model.initialize(ctx=ctx)\r\n",
      "    n_train_samples = train_mask.sum().asscalar()\r\n",
      "    loss_fcn = gluon.loss.SoftmaxCELoss()\r\n",
      "\r\n",
      "    # use optimizer\r\n",
      "    print(model.collect_params())\r\n",
      "    trainer = gluon.Trainer(model.collect_params(), 'adam',\r\n",
      "            {'learning_rate': args.lr, 'wd': args.weight_decay})\r\n",
      "\r\n",
      "    # initialize graph\r\n",
      "    dur = []\r\n",
      "    for epoch in range(args.n_epochs):\r\n",
      "        if epoch >= 3:\r\n",
      "            t0 = time.time()\r\n",
      "        # forward\r\n",
      "        with mx.autograd.record():\r\n",
      "            pred = model(features)\r\n",
      "            loss = loss_fcn(pred, labels, mx.nd.expand_dims(train_mask, 1))\r\n",
      "            loss = loss.sum() / n_train_samples\r\n",
      "\r\n",
      "        loss.backward()\r\n",
      "        trainer.step(batch_size=1)\r\n",
      "\r\n",
      "        if epoch >= 3:\r\n",
      "            loss.asscalar()\r\n",
      "            dur.append(time.time() - t0)\r\n",
      "            acc = evaluate(model, features, labels, val_mask)\r\n",
      "            print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | Accuracy {:.4f} | \"\r\n",
      "                  \"ETputs(KTEPS) {:.2f}\". format(\r\n",
      "                epoch, np.mean(dur), loss.asscalar(), acc, n_edges / np.mean(dur) / 1000))\r\n",
      "\r\n",
      "    # test set accuracy\r\n",
      "    acc = evaluate(model, features, labels, test_mask)\r\n",
      "    print(\"Test accuracy {:.2%}\".format(acc))\r\n",
      "\r\n",
      "    model.save_parameters(args.save_path)\r\n",
      "\r\n",
      "def parse_args():\r\n",
      "    parser = argparse.ArgumentParser(description='GCN')\r\n",
      "    register_data_args(parser)\r\n",
      "    parser.add_argument(\"--dropout\", type=float, default=0.5,\r\n",
      "            help=\"dropout probability\")\r\n",
      "    parser.add_argument(\"--gpu\", type=int, default=-1,\r\n",
      "            help=\"gpu\")\r\n",
      "    parser.add_argument(\"--lr\", type=float, default=3e-2,\r\n",
      "            help=\"learning rate\")\r\n",
      "    parser.add_argument(\"--n-epochs\", type=int, default=200,\r\n",
      "            help=\"number of training epochs\")\r\n",
      "    parser.add_argument(\"--n-hidden\", type=int, default=16,\r\n",
      "            help=\"number of hidden gcn units\")\r\n",
      "    parser.add_argument(\"--n-layers\", type=int, default=1,\r\n",
      "            help=\"number of hidden gcn layers\")\r\n",
      "    parser.add_argument(\"--weight-decay\", type=float, default=5e-4,\r\n",
      "            help=\"Weight for L2 loss\")\r\n",
      "    parser.add_argument(\"--self-loop\", action='store_true',\r\n",
      "            help=\"graph self-loop (default=False)\")\r\n",
      "    parser.add_argument(\"--save-path\", type=str, default='./model/gcn.params',\r\n",
      "            help=\"path to save model\")\r\n",
      "    parser.set_defaults(self_loop=False)\r\n",
      "\r\n",
      "    return parser.parse_args()\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    args = parse_args()\r\n",
      "    num_gpus = int(os.environ['SM_NUM_GPUS'])\r\n",
      "    if num_gpus == 0:\r\n",
      "        args.gpu = -1\r\n",
      "    else:\r\n",
      "        args.gpu = 0\r\n",
      "\r\n",
      "    path = str(os.environ['SM_MODEL_DIR'])\r\n",
      "    args.save_path = os.path.join(path, 'gcn.params')\r\n",
      "    print(args)\r\n",
      "    main(args)\r\n"
     ]
    }
   ],
   "source": [
    "!cat mxnet_gcn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker's  estimator class\n",
    "The SageMaker Estimator allows us to run single machine in SageMaker, using CPU or GPU-based instances.\n",
    "\n",
    "When we create the estimator, we pass in the filename of our training script, the name of our IAM execution role. We also provide a few other parameters. train_instance_count and train_instance_type determine the number and type of SageMaker instances that will be used for the training job. The hyperparameters parameter is a dict of values that will be passed to your training script -- you can see how to access these values in the mxnet_gcn.py script above.\n",
    "\n",
    "Here we can use the official docker image for this example, please see https://github.com/aws/sagemaker-mxnet-container for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No framework_version specified, defaulting to version 1.2. This is not the latest supported version. If you would like to use version 1.4.1, please add framework_version=1.4.1 to your constructor.\n",
      "The Python 2 mxnet images will be soon deprecated and may not be supported for newer upcoming versions of the mxnet images.\n",
      "Please set the argument \"py_version='py3'\" to use the Python 3 mxnet image.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397262719838.dkr.ecr.us-east-2.amazonaws.com/beta-mxnet-training:1.6.0-py3-gpu-build\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.mxnet.estimator import MXNet\n",
    "\n",
    "CODE_PATH = 'mxnet_gcn.py'\n",
    "\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "docker_name = 'beta-mxnet-training'\n",
    "docker_tag = '1.6.0-py3-gpu-build'\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/{}:{}'.format(account, region, docker_name, docker_tag)\n",
    "print(image)\n",
    "\n",
    "params = {}\n",
    "params['dataset'] = 'cora'\n",
    "estimator = MXNet(entry_point=CODE_PATH,\n",
    "                        role=role, \n",
    "                        train_instance_count=1, \n",
    "                        train_instance_type='ml.p3.2xlarge',\n",
    "                        image_name=image,\n",
    "                        hyperparameters=params,\n",
    "                        sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Training Job\n",
    "After we've constructed our Estimator object, we can fit it using sagemaker (The dataset will be automatically downloaded). Below we run SageMaker training on one channels: training-code, the code to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-24 13:57:26 Starting - Starting the training job...\n",
      "2019-11-24 13:57:27 Starting - Launching requested ML instances...\n",
      "2019-11-24 13:58:20 Starting - Preparing the instances for training......\n",
      "2019-11-24 13:59:13 Downloading - Downloading input data\n",
      "2019-11-24 13:59:13 Training - Downloading the training image.........\n",
      "2019-11-24 14:00:50 Training - Training image download completed. Training in progress.."
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "You can get the model training output from the Sagemaker Console by searching for the training task and looking for the address of 'S3 model artifact'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_dgl_py36_mxnet1.5",
   "language": "python",
   "name": "conda_dgl_py36_mxnet1.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
